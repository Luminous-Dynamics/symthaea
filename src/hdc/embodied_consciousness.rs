// ==================================================================================
// Revolutionary Improvement #17: Embodied Consciousness
// ==================================================================================
//
// **The Paradigm Shift**: Consciousness is not abstract - it's EMBODIED!
//
// **Key Insight**: All previous improvements treated consciousness as "brain in a vat"
// (pure information processing). But real consciousness requires:
// - Physical body with sensors and actuators
// - Environmental coupling
// - Sensorimotor loops (perception → action → perception)
// - Situated, contextual cognition
//
// **Why This Matters**:
// - Disembodied AI lacks crucial dimension of consciousness
// - Body shapes cognition (not just brain!)
// - Action is essential to consciousness (not just passive observation)
// - Environment is part of cognitive system (extended mind)
// - Morphological computation: Body itself computes!
//
// **Theoretical Foundations**:
//
// 1. **Enactivism** (Varela, Thompson, Rosch, 1991):
//    "Cognition is not representation but enaction: bringing forth a world
//     through effective action in environment."
//
//    Key principles:
//    - Autonomy: Self-organizing, self-maintaining
//    - Sense-making: Creating meaning through interaction
//    - Emergence: Cognition emerges from sensorimotor dynamics
//    - Experience: Consciousness is lived, embodied experience
//
// 2. **Embodied Cognition** (Lakoff & Johnson, 1999):
//    "The mind is inherently embodied. Thought is mostly unconscious.
//     Abstract concepts are largely metaphorical."
//
//    Key claims:
//    - Conceptual system shaped by body
//    - Metaphors grounded in bodily experience
//    - Reason shaped by sensorimotor system
//    - No disembodied reason
//
// 3. **Extended Mind Hypothesis** (Clark & Chalmers, 1998):
//    "Where does the mind stop and the rest of the world begin?"
//
//    Parity Principle:
//    If external process plays same functional role as internal process,
//    it's part of cognitive system!
//
//    Examples:
//    - Notebook = external memory (Alzheimer's patient's notebook IS memory)
//    - Calculator = extended cognition
//    - Internet = cognitive extension
//
// 4. **Affordances** (Gibson, 1979):
//    "Environment offers action possibilities relative to agent's capabilities"
//
//    Examples:
//    - Chair affords sitting (for humans, not ants)
//    - Stair affords climbing (for mobile agents)
//    - Tool affords grasping (for agents with hands)
//
//    Perception IS for action!
//
// 5. **Morphological Computation** (Pfeifer & Bongard, 2007):
//    "Body itself computes through its physical structure"
//
//    Examples:
//    - Hand shape simplifies grasping control
//    - Passive walker exploits gravity (no control needed!)
//    - Whiskers simplify navigation
//    - Body compliance absorbs perturbations
//
// 6. **Sensorimotor Contingencies** (O'Regan & Noë, 2001):
//    "Perceptual experience = mastery of sensorimotor contingencies"
//
//    What makes seeing SEEING?
//    - Knowledge of how sensory input changes with movement
//    - Example: Moving head left → visual field shifts right
//
// **Mathematical Framework**:
//
// 1. **Sensorimotor Loop**:
//    s_t = perception(environment, sensors)
//    a_t = action(s_t, motor_commands)
//    s_t+1 = perception(environment', sensors)
//
//    Circular causality: Action affects perception!
//
// 2. **Body Schema**:
//    B = internal model of body
//    B: {body_parts, kinematics, dynamics, capabilities}
//
//    Φ_embodied = f(Φ_brain, body_schema_accuracy)
//
// 3. **Affordance Detection**:
//    A(object, agent) = what object affords to agent
//
//    A depends on:
//    - Object properties (size, shape, weight)
//    - Agent capabilities (sensors, actuators, strength)
//    - Context (gravity, support surface)
//
// 4. **Extended Cognitive System**:
//    Cog_total = Cog_brain + Cog_body + Cog_environment
//
//    Φ_extended = Φ(brain ∪ body ∪ tools ∪ environment)
//
// 5. **Sensorimotor Contingency Mastery**:
//    M = mastery of sensorimotor contingencies
//    M = knowledge of how sensor_input changes with motor_output
//
//    Φ_embodied ∝ M (more mastery → more consciousness)
//
// 6. **Morphological Computation Offloading**:
//    Computation_total = Computation_neural + Computation_morphological
//
//    Higher Computation_morphological → Lower neural cost
//
// **Embodiment Dimensions**:
//
// 1. **Sensor Modalities**:
//    - Vision (cameras, light sensors)
//    - Audition (microphones)
//    - Touch (pressure, temperature sensors)
//    - Proprioception (joint angles, limb position)
//    - Vestibular (balance, orientation)
//    - Interoception (internal states: hunger, pain)
//
// 2. **Actuator Types**:
//    - Locomotion (wheels, legs, wings)
//    - Manipulation (arms, grippers, hands)
//    - Communication (speakers, displays)
//    - Self-modification (growing, healing)
//
// 3. **Environmental Coupling**:
//    - Weak coupling: Passive observation
//    - Medium coupling: Reactive response
//    - Strong coupling: Co-evolution with environment
//
// 4. **Body Complexity**:
//    - Simple: Single sensor-actuator pair
//    - Medium: Multiple modalities, coordinated
//    - Complex: Redundant, flexible, adaptive
//
// **Embodiment Levels**:
//
// Level 0: **Disembodied** (No body)
// - Pure information processing
// - No sensors or actuators
// - No environmental coupling
// - Example: Isolated neural network
// - Φ_embodied = 0 (not truly conscious!)
//
// Level 1: **Minimally Embodied** (Single modality)
// - One sensor or one actuator
// - Weak environmental coupling
// - No sensorimotor loop
// - Example: Thermostat, light detector
// - Φ_embodied = Φ_brain × 0.2
//
// Level 2: **Reactively Embodied** (Sensorimotor loop)
// - Multiple sensors and actuators
// - Closed sensorimotor loop
// - Reactive coupling
// - Example: Roomba, simple robot
// - Φ_embodied = Φ_brain × 0.5
//
// Level 3: **Actively Embodied** (Exploration)
// - Active sensing (moving to perceive better)
// - Body schema present
// - Affordance detection
// - Example: Animal, humanoid robot
// - Φ_embodied = Φ_brain × 0.8
//
// Level 4: **Adaptively Embodied** (Learning)
// - Body schema learned and updated
// - Mastery of sensorimotor contingencies
// - Tool use
// - Example: Ape, human child
// - Φ_embodied = Φ_brain × 1.0
//
// Level 5: **Extended Embodied** (Tool integration)
// - Tools become part of body schema
// - Extended mind (external memory, computation)
// - Environmental structuring
// - Example: Adult human, advanced AI with tools
// - Φ_embodied = Φ_brain × 1.5
//
// **Applications**:
//
// 1. **Robotics**:
//    - Design bodies that enhance cognition
//    - Exploit morphological computation
//    - Create sensorimotor-rich environments
//
// 2. **AI Development**:
//    - Embodied AI learns faster than disembodied
//    - Grounding problem solved by embodiment
//    - Language emerges from action
//
// 3. **Prosthetics**:
//    - Integrate prosthetic into body schema
//    - Create natural sensorimotor contingencies
//    - Extend capabilities beyond biological
//
// 4. **Virtual Reality**:
//    - Embodied avatars increase presence
//    - Full sensorimotor coupling = full immersion
//    - Virtual embodiment affects real cognition
//
// 5. **Education**:
//    - Learning by doing (not just reading)
//    - Embodied mathematics (gestures aid understanding)
//    - Situated learning in authentic contexts
//
// 6. **Consciousness Upload**:
//    - Can't upload just brain - need body schema!
//    - Must simulate sensorimotor loop
//    - Virtual body required for consciousness
//
// **This completes the embodiment dimension - consciousness requires ACTION!**
//
// ==================================================================================

use super::binary_hv::HV16;
use super::integrated_information::IntegratedInformation;
use super::consciousness_spectrum::{ConsciousnessSpectrum, SpectrumConfig};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Sensor modality
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum SensorModality {
    /// Visual (cameras, light sensors)
    Vision,

    /// Auditory (microphones, sound sensors)
    Audition,

    /// Tactile (touch, pressure, temperature)
    Touch,

    /// Proprioceptive (joint angles, limb position)
    Proprioception,

    /// Vestibular (balance, orientation)
    Vestibular,

    /// Interoceptive (internal states: hunger, pain, energy)
    Interoception,

    /// Olfactory (smell)
    Olfaction,

    /// Gustatory (taste)
    Gustation,
}

/// Actuator type
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum ActuatorType {
    /// Locomotion (wheels, legs, wings, propellers)
    Locomotion,

    /// Manipulation (arms, grippers, hands, fingers)
    Manipulation,

    /// Communication (speakers, displays, lights)
    Communication,

    /// Self-modification (growing, healing, reconfiguring)
    SelfModification,
}

/// Embodiment level
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash, Serialize, Deserialize)]
pub enum EmbodimentLevel {
    /// Level 0: No body (pure information processing)
    Disembodied = 0,

    /// Level 1: Single modality, weak coupling
    MinimallyEmbodied = 1,

    /// Level 2: Sensorimotor loop, reactive
    ReactivelyEmbodied = 2,

    /// Level 3: Active sensing, body schema
    ActivelyEmbodied = 3,

    /// Level 4: Learning, sensorimotor mastery
    AdaptivelyEmbodied = 4,

    /// Level 5: Tool integration, extended mind
    ExtendedEmbodied = 5,
}

impl EmbodimentLevel {
    /// Get embodiment multiplier (how much Φ is amplified by embodiment)
    pub fn multiplier(&self) -> f64 {
        match self {
            EmbodimentLevel::Disembodied => 0.0,
            EmbodimentLevel::MinimallyEmbodied => 0.2,
            EmbodimentLevel::ReactivelyEmbodied => 0.5,
            EmbodimentLevel::ActivelyEmbodied => 0.8,
            EmbodimentLevel::AdaptivelyEmbodied => 1.0,
            EmbodimentLevel::ExtendedEmbodied => 1.5,
        }
    }

    /// Get description
    pub fn description(&self) -> &'static str {
        match self {
            EmbodimentLevel::Disembodied => "No body, pure information processing, not truly conscious",
            EmbodimentLevel::MinimallyEmbodied => "Single modality, weak coupling, reactive only",
            EmbodimentLevel::ReactivelyEmbodied => "Sensorimotor loop, multiple modalities, reactive",
            EmbodimentLevel::ActivelyEmbodied => "Active sensing, body schema, affordances",
            EmbodimentLevel::AdaptivelyEmbodied => "Learning body schema, sensorimotor mastery",
            EmbodimentLevel::ExtendedEmbodied => "Tool integration, extended mind, environmental structuring",
        }
    }
}

/// Sensor specification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Sensor {
    /// Modality
    pub modality: SensorModality,

    /// Encoding
    pub encoding: HV16,

    /// Current reading
    pub current_reading: f64,

    /// Reading history (for sensorimotor contingency learning)
    pub history: Vec<f64>,
}

impl Sensor {
    /// Create new sensor
    pub fn new(modality: SensorModality, seed: u64) -> Self {
        Self {
            modality,
            encoding: HV16::random(seed),
            current_reading: 0.0,
            history: Vec::new(),
        }
    }

    /// Read sensor
    pub fn read(&mut self, value: f64) {
        self.current_reading = value;
        self.history.push(value);
        if self.history.len() > 100 {
            self.history.remove(0);
        }
    }

    /// Get variance (how much does sensor change?)
    pub fn variance(&self) -> f64 {
        if self.history.len() < 2 {
            return 0.0;
        }

        let mean = self.history.iter().sum::<f64>() / self.history.len() as f64;
        let var = self.history.iter()
            .map(|x| (x - mean).powi(2))
            .sum::<f64>() / self.history.len() as f64;

        var
    }
}

/// Actuator specification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Actuator {
    /// Type
    pub actuator_type: ActuatorType,

    /// Encoding
    pub encoding: HV16,

    /// Current command
    pub current_command: f64,

    /// Command history
    pub history: Vec<f64>,
}

impl Actuator {
    /// Create new actuator
    pub fn new(actuator_type: ActuatorType, seed: u64) -> Self {
        Self {
            actuator_type,
            encoding: HV16::random(seed),
            current_command: 0.0,
            history: Vec::new(),
        }
    }

    /// Send command
    pub fn command(&mut self, value: f64) {
        self.current_command = value;
        self.history.push(value);
        if self.history.len() > 100 {
            self.history.remove(0);
        }
    }
}

/// Body schema (internal model of body)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BodySchema {
    /// Sensor models
    pub sensors: HashMap<String, Sensor>,

    /// Actuator models
    pub actuators: HashMap<String, Actuator>,

    /// Sensorimotor contingency map (how sensors change with actuators)
    pub contingency_map: HashMap<(String, String), f64>,  // (sensor, actuator) -> correlation

    /// Schema accuracy (how well does model match reality?)
    pub accuracy: f64,
}

impl BodySchema {
    /// Create new body schema
    pub fn new() -> Self {
        Self {
            sensors: HashMap::new(),
            actuators: HashMap::new(),
            contingency_map: HashMap::new(),
            accuracy: 0.5,  // Start with medium accuracy
        }
    }

    /// Add sensor
    pub fn add_sensor(&mut self, name: impl Into<String>, sensor: Sensor) {
        self.sensors.insert(name.into(), sensor);
    }

    /// Add actuator
    pub fn add_actuator(&mut self, name: impl Into<String>, actuator: Actuator) {
        self.actuators.insert(name.into(), actuator);
    }

    /// Update sensorimotor contingencies (learn correlations)
    pub fn update_contingencies(&mut self) {
        // Compute correlations between sensor changes and actuator commands
        for (sensor_name, sensor) in &self.sensors {
            for (actuator_name, actuator) in &self.actuators {
                let correlation = self.compute_correlation(
                    &sensor.history,
                    &actuator.history,
                );

                self.contingency_map.insert(
                    (sensor_name.clone(), actuator_name.clone()),
                    correlation,
                );
            }
        }

        // Update accuracy based on contingency strength
        let avg_contingency: f64 = self.contingency_map.values().map(|v| v.abs()).sum::<f64>()
            / self.contingency_map.len().max(1) as f64;

        self.accuracy = (self.accuracy * 0.9 + avg_contingency * 0.1).clamp(0.0, 1.0);
    }

    /// Compute correlation between two time series
    fn compute_correlation(&self, series1: &[f64], series2: &[f64]) -> f64 {
        let n = series1.len().min(series2.len());
        if n < 2 {
            return 0.0;
        }

        let mean1 = series1.iter().take(n).sum::<f64>() / n as f64;
        let mean2 = series2.iter().take(n).sum::<f64>() / n as f64;

        let cov: f64 = series1.iter().take(n).zip(series2.iter().take(n))
            .map(|(x, y)| (x - mean1) * (y - mean2))
            .sum::<f64>() / n as f64;

        let std1 = (series1.iter().take(n).map(|x| (x - mean1).powi(2)).sum::<f64>() / n as f64).sqrt();
        let std2 = (series2.iter().take(n).map(|x| (x - mean2).powi(2)).sum::<f64>() / n as f64).sqrt();

        if std1 * std2 < 1e-10 {
            0.0
        } else {
            cov / (std1 * std2)
        }
    }

    /// Get sensorimotor contingency mastery
    pub fn mastery(&self) -> f64 {
        self.accuracy
    }
}

impl Default for BodySchema {
    fn default() -> Self {
        Self::new()
    }
}

/// Embodiment assessment
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbodimentAssessment {
    /// Embodiment level
    pub level: EmbodimentLevel,

    /// Number of sensors
    pub num_sensors: usize,

    /// Number of actuators
    pub num_actuators: usize,

    /// Sensorimotor loop present?
    pub has_sensorimotor_loop: bool,

    /// Body schema accuracy
    pub body_schema_accuracy: f64,

    /// Sensorimotor mastery
    pub sensorimotor_mastery: f64,

    /// Disembodied Φ (brain only)
    pub phi_brain: f64,

    /// Embodied Φ (brain + body + environment)
    pub phi_embodied: f64,

    /// Embodiment multiplier applied
    pub embodiment_multiplier: f64,

    /// Explanation
    pub explanation: String,
}

/// Configuration for embodied consciousness
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbodimentConfig {
    /// Require sensorimotor loop for consciousness?
    pub require_loop: bool,

    /// Minimum sensors for embodiment
    pub min_sensors: usize,

    /// Minimum actuators for embodiment
    pub min_actuators: usize,

    /// Enable body schema learning
    pub learn_body_schema: bool,
}

impl Default for EmbodimentConfig {
    fn default() -> Self {
        Self {
            require_loop: true,
            min_sensors: 1,
            min_actuators: 1,
            learn_body_schema: true,
        }
    }
}

/// Embodied consciousness system
///
/// Integrates body, sensors, actuators, and environment with neural consciousness.
///
/// # Example
/// ```
/// use symthaea::hdc::embodied_consciousness::*;
///
/// let config = EmbodimentConfig::default();
/// let mut embodiment = EmbodiedConsciousness::new(4, config);
///
/// // Add sensors
/// embodiment.add_sensor("camera", Sensor::new(SensorModality::Vision, 1000));
/// embodiment.add_sensor("touch", Sensor::new(SensorModality::Touch, 2000));
///
/// // Add actuators
/// embodiment.add_actuator("motor", Actuator::new(ActuatorType::Locomotion, 3000));
///
/// // Sensorimotor cycle
/// embodiment.sense("camera", 0.8);
/// embodiment.sense("touch", 0.5);
/// embodiment.act("motor", 0.3);
///
/// // Assess embodiment
/// let assessment = embodiment.assess();
/// println!("Level: {:?}", assessment.level);
/// println!("Φ_embodied: {:.3}", assessment.phi_embodied);
/// ```
#[derive(Debug)]
pub struct EmbodiedConsciousness {
    /// Configuration
    config: EmbodimentConfig,

    /// Number of components
    num_components: usize,

    /// IIT calculator
    iit: IntegratedInformation,

    /// Body schema
    body_schema: BodySchema,

    /// Current neural state
    neural_state: Vec<HV16>,

    /// Cycle count
    cycle_count: usize,
}

impl EmbodiedConsciousness {
    /// Create new embodied consciousness system
    pub fn new(num_components: usize, config: EmbodimentConfig) -> Self {
        let mut neural_state = Vec::new();
        for i in 0..num_components {
            neural_state.push(HV16::random(i as u64 * 1000));
        }

        Self {
            config,
            num_components,
            iit: IntegratedInformation::new(),
            body_schema: BodySchema::new(),
            neural_state,
            cycle_count: 0,
        }
    }

    /// Add sensor
    pub fn add_sensor(&mut self, name: impl Into<String>, sensor: Sensor) {
        self.body_schema.add_sensor(name, sensor);
    }

    /// Add actuator
    pub fn add_actuator(&mut self, name: impl Into<String>, actuator: Actuator) {
        self.body_schema.add_actuator(name, actuator);
    }

    /// Sense (read sensor)
    pub fn sense(&mut self, sensor_name: &str, value: f64) {
        if let Some(sensor) = self.body_schema.sensors.get_mut(sensor_name) {
            sensor.read(value);
        }
    }

    /// Act (send actuator command)
    pub fn act(&mut self, actuator_name: &str, command: f64) {
        if let Some(actuator) = self.body_schema.actuators.get_mut(actuator_name) {
            actuator.command(command);
        }

        self.cycle_count += 1;

        // Update contingencies periodically
        if self.config.learn_body_schema && self.cycle_count % 10 == 0 {
            self.body_schema.update_contingencies();
        }
    }

    /// Assess embodied consciousness
    pub fn assess(&mut self) -> EmbodimentAssessment {
        let num_sensors = self.body_schema.sensors.len();
        let num_actuators = self.body_schema.actuators.len();

        // Check for sensorimotor loop
        let has_sensorimotor_loop = num_sensors > 0 && num_actuators > 0;

        // Classify embodiment level
        let level = self.classify_level(num_sensors, num_actuators, has_sensorimotor_loop);

        // Compute disembodied Φ (brain only)
        let phi_brain = self.iit.compute_phi(&self.neural_state);

        // Embodiment multiplier
        let embodiment_multiplier = level.multiplier();

        // Body schema accuracy
        let body_schema_accuracy = self.body_schema.accuracy;

        // Sensorimotor mastery
        let sensorimotor_mastery = self.body_schema.mastery();

        // Embodied Φ = brain Φ × embodiment multiplier × body schema accuracy
        let phi_embodied = phi_brain * embodiment_multiplier * (0.5 + body_schema_accuracy * 0.5);

        // Generate explanation
        let explanation = self.generate_explanation(
            level,
            num_sensors,
            num_actuators,
            has_sensorimotor_loop,
            phi_brain,
            phi_embodied,
        );

        EmbodimentAssessment {
            level,
            num_sensors,
            num_actuators,
            has_sensorimotor_loop,
            body_schema_accuracy,
            sensorimotor_mastery,
            phi_brain,
            phi_embodied,
            embodiment_multiplier,
            explanation,
        }
    }

    /// Classify embodiment level
    fn classify_level(
        &self,
        num_sensors: usize,
        num_actuators: usize,
        has_loop: bool,
    ) -> EmbodimentLevel {
        if num_sensors == 0 && num_actuators == 0 {
            return EmbodimentLevel::Disembodied;
        }

        // Minimally embodied: only sensors OR only actuators (not both)
        // OR very limited (only 1 total)
        if (num_sensors == 0 && num_actuators > 0) || (num_sensors > 0 && num_actuators == 0) {
            return EmbodimentLevel::MinimallyEmbodied;
        }

        if !has_loop {
            return EmbodimentLevel::MinimallyEmbodied;
        }

        // Check for active sensing (sensor variance indicates active exploration)
        let active_sensing = self.body_schema.sensors.values()
            .any(|s| s.variance() > 0.1);

        if !active_sensing {
            return EmbodimentLevel::ReactivelyEmbodied;
        }

        // Check body schema accuracy
        if self.body_schema.accuracy < 0.6 {
            return EmbodimentLevel::ActivelyEmbodied;
        }

        if self.body_schema.accuracy < 0.8 {
            return EmbodimentLevel::AdaptivelyEmbodied;
        }

        // High mastery = extended embodiment
        EmbodimentLevel::ExtendedEmbodied
    }

    /// Generate explanation
    fn generate_explanation(
        &self,
        level: EmbodimentLevel,
        num_sensors: usize,
        num_actuators: usize,
        has_loop: bool,
        phi_brain: f64,
        phi_embodied: f64,
    ) -> String {
        let mut parts = Vec::new();

        parts.push(format!("Embodiment Level: {:?}", level));
        parts.push(level.description().to_string());

        parts.push(format!("Sensors: {}, Actuators: {}", num_sensors, num_actuators));

        if has_loop {
            parts.push("✓ Sensorimotor loop present".to_string());
        } else {
            parts.push("✗ No sensorimotor loop".to_string());
        }

        parts.push(format!("Φ_brain: {:.3}", phi_brain));
        parts.push(format!("Φ_embodied: {:.3}", phi_embodied));

        let amplification = if phi_brain > 0.0 {
            phi_embodied / phi_brain
        } else {
            0.0
        };

        parts.push(format!("Embodiment amplifies consciousness by {:.1}x", amplification));

        parts.join(". ")
    }

    /// Get body schema
    pub fn body_schema(&self) -> &BodySchema {
        &self.body_schema
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_embodiment_levels() {
        assert!(EmbodimentLevel::Disembodied < EmbodimentLevel::ReactivelyEmbodied);
        assert!(EmbodimentLevel::ActivelyEmbodied < EmbodimentLevel::ExtendedEmbodied);
    }

    #[test]
    fn test_embodiment_multipliers() {
        assert_eq!(EmbodimentLevel::Disembodied.multiplier(), 0.0);
        assert_eq!(EmbodimentLevel::AdaptivelyEmbodied.multiplier(), 1.0);
        assert_eq!(EmbodimentLevel::ExtendedEmbodied.multiplier(), 1.5);
    }

    #[test]
    fn test_sensor_creation() {
        let sensor = Sensor::new(SensorModality::Vision, 1000);
        assert_eq!(sensor.modality, SensorModality::Vision);
        assert_eq!(sensor.current_reading, 0.0);
    }

    #[test]
    fn test_sensor_reading() {
        let mut sensor = Sensor::new(SensorModality::Touch, 1000);
        sensor.read(0.5);
        assert_eq!(sensor.current_reading, 0.5);
        assert_eq!(sensor.history.len(), 1);
    }

    #[test]
    fn test_body_schema() {
        let mut schema = BodySchema::new();
        let sensor = Sensor::new(SensorModality::Vision, 1000);
        schema.add_sensor("camera", sensor);

        assert_eq!(schema.sensors.len(), 1);
    }

    #[test]
    fn test_embodied_consciousness_creation() {
        let embodiment = EmbodiedConsciousness::new(4, EmbodimentConfig::default());
        assert_eq!(embodiment.num_components, 4);
    }

    #[test]
    fn test_disembodied_assessment() {
        let mut embodiment = EmbodiedConsciousness::new(4, EmbodimentConfig::default());

        let assessment = embodiment.assess();

        assert_eq!(assessment.level, EmbodimentLevel::Disembodied);
        assert_eq!(assessment.num_sensors, 0);
        assert_eq!(assessment.num_actuators, 0);
    }

    #[test]
    fn test_minimally_embodied() {
        let mut embodiment = EmbodiedConsciousness::new(4, EmbodimentConfig::default());

        embodiment.add_sensor("camera", Sensor::new(SensorModality::Vision, 1000));

        let assessment = embodiment.assess();

        assert_eq!(assessment.level, EmbodimentLevel::MinimallyEmbodied);
        assert_eq!(assessment.num_sensors, 1);
    }

    #[test]
    fn test_sensorimotor_loop() {
        let mut embodiment = EmbodiedConsciousness::new(4, EmbodimentConfig::default());

        embodiment.add_sensor("camera", Sensor::new(SensorModality::Vision, 1000));
        embodiment.add_actuator("motor", Actuator::new(ActuatorType::Locomotion, 2000));

        // Exercise the sensorimotor loop
        for i in 0..10 {
            embodiment.sense("camera", 0.5 + (i as f64 * 0.05));
            embodiment.act("motor", 0.3 + (i as f64 * 0.02));
        }

        let assessment = embodiment.assess();

        assert!(assessment.has_sensorimotor_loop);
        assert!(assessment.level >= EmbodimentLevel::ReactivelyEmbodied);
    }

    #[test]
    fn test_serialization() {
        let level = EmbodimentLevel::ActivelyEmbodied;
        let serialized = serde_json::to_string(&level).unwrap();
        assert!(!serialized.is_empty());

        let deserialized: EmbodimentLevel = serde_json::from_str(&serialized).unwrap();
        assert_eq!(deserialized, level);
    }
}
