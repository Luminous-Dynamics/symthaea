//! Learning Integration Module
//!
//! This module bridges the LearnableLTC neural network with the ContinuousMind,
//! enabling gradient-based adaptation from experience.
//!
//! ## Architecture
//!
//! ```text
//! ┌────────────────────────────────────────────────────────────────────┐
//! │                    LEARNING INTEGRATION                            │
//! ├────────────────────────────────────────────────────────────────────┤
//! │                                                                    │
//! │  ┌─────────────────┐     ┌─────────────────┐     ┌─────────────┐  │
//! │  │  Active         │     │   Learnable     │     │  Memory     │  │
//! │  │  Inference      │────▶│   LTC           │────▶│  Consol.    │  │
//! │  │  (Pred. Error)  │     │   (Adaptation)  │     │  (Sleep)    │  │
//! │  └─────────────────┘     └─────────────────┘     └─────────────┘  │
//! │         │                       ▲                       │         │
//! │         │                       │                       │         │
//! │         └───────────────────────┼───────────────────────┘         │
//! │                                 │                                 │
//! │                    ┌─────────────────────┐                        │
//! │                    │  Neuromodulation    │                        │
//! │                    │  (Learning Rate)    │                        │
//! │                    └─────────────────────┘                        │
//! └────────────────────────────────────────────────────────────────────┘
//! ```
//!
//! ## Key Innovation
//!
//! The LTC learns from **prediction errors** generated by Active Inference:
//! - High prediction error → stronger gradient signal
//! - Low prediction error → minimal updates (system understands the pattern)
//!
//! Neuromodulation adjusts learning dynamically:
//! - High dopamine → increased learning rate (reward signal)
//! - High acetylcholine → enhanced plasticity (attention signal)
//! - High stress → reduced learning (protective mechanism)

use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::sync::{Arc, Mutex};

use crate::learnable_ltc::{LearnableLTC, LearnableLTCConfig, LTCTrainingStats};
use crate::hdc::real_hv::RealHV;
use crate::hdc::HDC_DIMENSION;

/// Learning integration configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LearningConfig {
    /// Base learning rate (modified by neuromodulation)
    pub base_lr: f32,

    /// Minimum learning rate (floor)
    pub min_lr: f32,

    /// Maximum learning rate (ceiling)
    pub max_lr: f32,

    /// Number of neurons in the LTC
    pub num_neurons: usize,

    /// Input dimension (from HDC state encoding)
    pub input_dim: usize,

    /// Output dimension (action/prediction space)
    pub output_dim: usize,

    /// Train every N cognitive cycles
    pub train_interval: usize,

    /// Batch size for training
    pub batch_size: usize,

    /// Experience buffer size
    pub buffer_size: usize,

    /// Minimum prediction error to trigger learning
    pub min_error_threshold: f32,

    /// Enable sleep-based consolidation
    pub enable_consolidation: bool,

    /// Consolidation replay multiplier during sleep
    pub sleep_replay_multiplier: usize,
}

impl Default for LearningConfig {
    fn default() -> Self {
        Self {
            base_lr: 0.001,
            min_lr: 0.0001,
            max_lr: 0.01,
            num_neurons: 512,       // Smaller than default for faster iteration
            input_dim: 256,         // Compressed HDC state
            output_dim: 64,         // Action/prediction space
            train_interval: 10,     // Train every 10 cycles
            batch_size: 16,
            buffer_size: 1000,
            min_error_threshold: 0.01,
            enable_consolidation: true,
            sleep_replay_multiplier: 3,
        }
    }
}

/// Experience tuple for replay buffer
#[derive(Debug, Clone)]
pub struct Experience {
    /// Compressed HDC state at time t
    pub state: Vec<f32>,

    /// Action/prediction made
    pub action: Vec<f32>,

    /// Resulting state at time t+1
    pub next_state: Vec<f32>,

    /// Prediction error (surprise)
    pub error: f32,

    /// Reward signal (if any)
    pub reward: f32,

    /// Importance weight (for prioritized replay)
    pub importance: f32,
}

/// Learning statistics
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct LearningStats {
    /// Total training steps
    pub total_steps: usize,

    /// Average loss over recent window
    pub avg_loss: f32,

    /// Current effective learning rate
    pub effective_lr: f32,

    /// Buffer utilization (0.0 - 1.0)
    pub buffer_utilization: f32,

    /// Average prediction error
    pub avg_prediction_error: f32,

    /// Time constant distribution (mean, std, min, max)
    pub tau_distribution: (f32, f32, f32, f32),

    /// Consciousness level from LTC activity
    pub consciousness_level: f32,

    /// Consolidation events (during sleep)
    pub consolidation_events: usize,
}

/// Learning Integration Engine
///
/// Bridges LearnableLTC with the broader consciousness architecture
pub struct LearningEngine {
    /// Configuration
    config: LearningConfig,

    /// The learnable neural network
    ltc: LearnableLTC,

    /// Experience replay buffer
    buffer: Vec<Experience>,

    /// Current position in buffer (circular)
    buffer_pos: usize,

    /// Cycle counter for training interval
    cycle_count: usize,

    /// Learning statistics
    stats: LearningStats,

    /// Current neuromodulation state
    dopamine: f32,
    acetylcholine: f32,
    stress: f32,

    /// Is currently in sleep consolidation mode
    is_consolidating: bool,
}

impl LearningEngine {
    /// Create new learning engine
    pub fn new(config: LearningConfig) -> Result<Self> {
        let ltc_config = LearnableLTCConfig {
            num_neurons: config.num_neurons,
            input_dim: config.input_dim,
            output_dim: config.output_dim,
            lr_weights: config.base_lr,
            lr_tau: config.base_lr * 0.1,  // Slower tau learning
            lr_bias: config.base_lr,
            dt: 0.01,
            num_steps: 50,  // Integration steps per forward pass
            tau_min: 0.1,
            tau_max: 10.0,
            sparsity: 0.15,  // 15% connectivity
            l2_reg: 0.0001,
            grad_clip: 1.0,
        };

        let ltc = LearnableLTC::new(ltc_config)?;
        let buffer_size = config.buffer_size;

        Ok(Self {
            config,
            ltc,
            buffer: Vec::with_capacity(buffer_size),
            buffer_pos: 0,
            cycle_count: 0,
            stats: LearningStats::default(),
            dopamine: 0.5,
            acetylcholine: 0.5,
            stress: 0.2,
            is_consolidating: false,
        })
    }

    /// Compress HDC state to input dimension
    fn compress_hdc_state(&self, states: &[RealHV]) -> Vec<f32> {
        if states.is_empty() {
            return vec![0.0; self.config.input_dim];
        }

        // Bundle all states
        let bundled = RealHV::bundle(states);

        // Downsample to input_dim by taking evenly spaced samples
        let step = HDC_DIMENSION / self.config.input_dim;
        bundled.values.iter()
            .step_by(step)
            .take(self.config.input_dim)
            .cloned()
            .collect()
    }

    /// Update neuromodulation state (called from ContinuousMind)
    pub fn update_neuromodulation(&mut self, dopamine: f32, acetylcholine: f32, stress: f32) {
        self.dopamine = dopamine.clamp(0.0, 1.0);
        self.acetylcholine = acetylcholine.clamp(0.0, 1.0);
        self.stress = stress.clamp(0.0, 1.0);
    }

    /// Compute effective learning rate based on neuromodulation
    fn effective_learning_rate(&self) -> f32 {
        // Dopamine increases learning (reward signal)
        let dopamine_mod = 0.5 + self.dopamine;  // 0.5 - 1.5x

        // Acetylcholine increases plasticity (attention)
        let ach_mod = 0.7 + self.acetylcholine * 0.6;  // 0.7 - 1.3x

        // Stress reduces learning (protective)
        let stress_mod = 1.0 - self.stress * 0.5;  // 0.5 - 1.0x

        let modulated = self.config.base_lr * dopamine_mod * ach_mod * stress_mod;
        modulated.clamp(self.config.min_lr, self.config.max_lr)
    }

    /// Add experience to replay buffer
    pub fn add_experience(&mut self, exp: Experience) {
        if self.buffer.len() < self.config.buffer_size {
            self.buffer.push(exp);
        } else {
            self.buffer[self.buffer_pos] = exp;
        }
        self.buffer_pos = (self.buffer_pos + 1) % self.config.buffer_size;
    }

    /// Create experience from cognitive state
    pub fn create_experience(
        &self,
        current_states: &[RealHV],
        action: &[f32],
        next_states: &[RealHV],
        prediction_error: f32,
        reward: f32,
    ) -> Experience {
        Experience {
            state: self.compress_hdc_state(current_states),
            action: action.to_vec(),
            next_state: self.compress_hdc_state(next_states),
            error: prediction_error,
            reward,
            importance: prediction_error.abs() + reward.abs() * 0.5,
        }
    }

    /// Process a cognitive cycle (called from continuous_mind)
    ///
    /// Returns (output prediction, should_train)
    pub fn process_cycle(
        &mut self,
        current_states: &[RealHV],
    ) -> Result<(Vec<f32>, bool)> {
        self.cycle_count += 1;

        // Compress state for LTC input
        let input = self.compress_hdc_state(current_states);

        // Forward pass through LTC
        let (output, _hidden_states) = self.ltc.forward(&input)?;

        // Check if we should train this cycle
        let should_train = self.cycle_count % self.config.train_interval == 0
            && self.buffer.len() >= self.config.batch_size;

        Ok((output, should_train))
    }

    /// Train on a batch from replay buffer
    pub fn train_batch(&mut self) -> Result<f32> {
        if self.buffer.len() < self.config.batch_size {
            return Ok(0.0);
        }

        // Sample batch (prioritized by importance)
        let mut indices: Vec<usize> = (0..self.buffer.len()).collect();
        indices.sort_by(|&a, &b| {
            self.buffer[b].importance.partial_cmp(&self.buffer[a].importance)
                .unwrap_or(std::cmp::Ordering::Equal)
        });

        let mut total_loss = 0.0f32;
        let batch_indices = &indices[..self.config.batch_size.min(self.buffer.len())];

        for &idx in batch_indices {
            let exp = &self.buffer[idx];

            // Skip if error is below threshold
            if exp.error.abs() < self.config.min_error_threshold {
                continue;
            }

            // Target: next_state (predict what comes next)
            let target = &exp.next_state;

            // Train step
            self.ltc.reset_state();
            let loss = self.ltc.train_step(&exp.state, target)?;
            total_loss += loss;
        }

        let avg_loss = total_loss / self.config.batch_size as f32;

        // Update stats
        self.stats.total_steps += 1;
        self.stats.avg_loss = self.stats.avg_loss * 0.95 + avg_loss * 0.05;  // EMA
        self.stats.effective_lr = self.effective_learning_rate();
        self.stats.buffer_utilization = self.buffer.len() as f32 / self.config.buffer_size as f32;
        self.stats.tau_distribution = self.ltc.get_tau_distribution();
        self.stats.consciousness_level = self.ltc.consciousness_level();

        // Compute average prediction error
        if !self.buffer.is_empty() {
            self.stats.avg_prediction_error = self.buffer.iter()
                .map(|e| e.error)
                .sum::<f32>() / self.buffer.len() as f32;
        }

        Ok(avg_loss)
    }

    /// Enter sleep consolidation mode
    pub fn begin_consolidation(&mut self) {
        if !self.config.enable_consolidation {
            return;
        }
        self.is_consolidating = true;
    }

    /// Perform memory consolidation (called during sleep)
    pub fn consolidate(&mut self) -> Result<f32> {
        if !self.is_consolidating || self.buffer.is_empty() {
            return Ok(0.0);
        }

        let mut total_loss = 0.0f32;
        let replay_count = self.config.sleep_replay_multiplier;

        // Replay important experiences multiple times
        for _ in 0..replay_count {
            let loss = self.train_batch()?;
            total_loss += loss;
        }

        self.stats.consolidation_events += replay_count;

        Ok(total_loss / replay_count as f32)
    }

    /// End sleep consolidation mode
    pub fn end_consolidation(&mut self) {
        self.is_consolidating = false;
    }

    /// Get current prediction from LTC state
    pub fn predict(&mut self, input_states: &[RealHV]) -> Result<Vec<f32>> {
        let input = self.compress_hdc_state(input_states);
        let (output, _) = self.ltc.forward(&input)?;
        Ok(output)
    }

    /// Get learning statistics
    pub fn stats(&self) -> &LearningStats {
        &self.stats
    }

    /// Get LTC training stats
    pub fn ltc_stats(&self) -> &LTCTrainingStats {
        self.ltc.stats()
    }

    /// Get current LTC state (for consciousness integration)
    pub fn get_ltc_state(&self) -> Vec<f32> {
        self.ltc.get_state()
    }

    /// Check if learning engine is active
    pub fn is_learning_active(&self) -> bool {
        self.buffer.len() >= self.config.batch_size
    }

    /// Serialize for persistence
    pub fn serialize(&self) -> Result<Vec<u8>> {
        let ltc_data = self.ltc.serialize()?;

        // Create serializable stats
        let stats_data = bincode::serialize(&self.stats)?;

        // Combine with length prefixes
        let mut data = Vec::new();
        data.extend_from_slice(&(ltc_data.len() as u64).to_le_bytes());
        data.extend_from_slice(&ltc_data);
        data.extend_from_slice(&stats_data);

        Ok(data)
    }

    /// Deserialize
    pub fn deserialize(config: LearningConfig, data: &[u8]) -> Result<Self> {
        if data.len() < 8 {
            anyhow::bail!("Data too short for deserialization");
        }

        // Read LTC length
        let ltc_len = u64::from_le_bytes(data[0..8].try_into()?) as usize;

        if data.len() < 8 + ltc_len {
            anyhow::bail!("Data too short for LTC");
        }

        // Deserialize LTC
        let ltc = LearnableLTC::deserialize(&data[8..8+ltc_len])?;

        // Deserialize stats
        let stats: LearningStats = bincode::deserialize(&data[8+ltc_len..])?;

        Ok(Self {
            config,
            ltc,
            buffer: Vec::new(),  // Buffer not persisted
            buffer_pos: 0,
            cycle_count: 0,
            stats,
            dopamine: 0.5,
            acetylcholine: 0.5,
            stress: 0.2,
            is_consolidating: false,
        })
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_learning_engine_creation() {
        let config = LearningConfig {
            num_neurons: 64,
            input_dim: 32,
            output_dim: 16,
            buffer_size: 100,
            ..Default::default()
        };

        let engine = LearningEngine::new(config).unwrap();
        assert_eq!(engine.buffer.len(), 0);
    }

    #[test]
    fn test_neuromodulation_learning_rate() {
        let config = LearningConfig {
            base_lr: 0.001,
            min_lr: 0.0001,
            max_lr: 0.01,
            num_neurons: 32,
            input_dim: 16,
            output_dim: 8,
            ..Default::default()
        };

        let mut engine = LearningEngine::new(config).unwrap();

        // High dopamine should increase learning rate
        engine.update_neuromodulation(0.9, 0.5, 0.1);
        let high_dopa_lr = engine.effective_learning_rate();

        // Low dopamine should decrease learning rate
        engine.update_neuromodulation(0.1, 0.5, 0.1);
        let low_dopa_lr = engine.effective_learning_rate();

        assert!(high_dopa_lr > low_dopa_lr,
            "High dopamine LR ({}) should be > low dopamine LR ({})",
            high_dopa_lr, low_dopa_lr);

        // High stress should reduce learning rate
        engine.update_neuromodulation(0.5, 0.5, 0.9);
        let high_stress_lr = engine.effective_learning_rate();

        engine.update_neuromodulation(0.5, 0.5, 0.1);
        let low_stress_lr = engine.effective_learning_rate();

        assert!(low_stress_lr > high_stress_lr,
            "Low stress LR ({}) should be > high stress LR ({})",
            low_stress_lr, high_stress_lr);
    }

    #[test]
    fn test_experience_buffer() {
        let config = LearningConfig {
            num_neurons: 32,
            input_dim: 16,
            output_dim: 8,
            buffer_size: 10,
            ..Default::default()
        };

        let mut engine = LearningEngine::new(config).unwrap();

        // Add experiences
        for i in 0..15 {
            let exp = Experience {
                state: vec![i as f32; 16],
                action: vec![0.0; 8],
                next_state: vec![(i + 1) as f32; 16],
                error: 0.5,
                reward: 0.0,
                importance: 0.5,
            };
            engine.add_experience(exp);
        }

        // Buffer should be capped at size
        assert_eq!(engine.buffer.len(), 10);
    }

    #[test]
    fn test_compress_hdc_state() {
        let config = LearningConfig {
            num_neurons: 32,
            input_dim: 64,
            output_dim: 16,
            ..Default::default()
        };

        let engine = LearningEngine::new(config).unwrap();

        // Create test HDC states
        let states: Vec<RealHV> = (0..3)
            .map(|i| RealHV::random(HDC_DIMENSION, i as u64))
            .collect();

        let compressed = engine.compress_hdc_state(&states);

        assert_eq!(compressed.len(), 64);
    }

    #[test]
    fn test_training_cycle() {
        let config = LearningConfig {
            num_neurons: 32,
            input_dim: 16,
            output_dim: 8,
            buffer_size: 50,
            batch_size: 8,
            train_interval: 5,
            min_error_threshold: 0.001,
            ..Default::default()
        };

        let mut engine = LearningEngine::new(config).unwrap();

        // Add experiences
        for i in 0..20 {
            let exp = Experience {
                state: vec![i as f32 / 20.0; 16],
                action: vec![0.5; 8],
                next_state: vec![(i + 1) as f32 / 20.0; 16],
                error: 0.5,
                reward: 0.1,
                importance: 0.5,
            };
            engine.add_experience(exp);
        }

        // Train a batch
        let loss = engine.train_batch().unwrap();

        println!("Training loss: {}", loss);
        assert!(engine.stats.total_steps > 0);
    }
}
