//! Revolutionary Improvement #48: Adaptive Primitive Selection via RL
//!
//! **The Breakthrough**: The system learns which primitives lead to better reasoning!
//!
//! This demo:
//! 1. Shows baseline performance (greedy selection)
//! 2. Demonstrates RL learning over episodes
//! 3. Tracks improvement in reasoning quality
//! 4. Visualizes learning curves

use anyhow::Result;
use symthaea::consciousness::{
    adaptive_reasoning::{AdaptiveReasoner, AgentStats},
    primitive_reasoning::PrimitiveReasoner,
};
use symthaea::hdc::{HV16, primitive_system::PrimitiveTier};
use serde_json;
use std::fs::File;
use std::io::Write;

fn main() -> Result<()> {
    println!("\nğŸŒŸ Revolutionary Improvement #48: Adaptive Primitive Selection via RL");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("The Breakthrough:");
    println!("  Primitive selection now LEARNS from experience!");
    println!();
    println!("  Before: Static greedy selection (maximize immediate Î¦)");
    println!("          Same strategy for all problems");
    println!();
    println!("  After:  Reinforcement learning (maximize long-term value)");
    println!("          Learns task-specific strategies!");
    println!();

    println!("Step 1: Baseline Performance (Greedy)");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    // Create baseline reasoner
    let mut baseline = PrimitiveReasoner::new();

    // Test on 5 different questions
    let test_questions: Vec<_> = (0..5)
        .map(|i| HV16::random(100 + i))
        .collect();

    println!("Running baseline (greedy) on 5 test questions...\n");

    let mut baseline_results = Vec::new();
    for (i, question) in test_questions.iter().enumerate() {
        let chain = baseline.reason(question.clone(), 10)?;
        println!("Question {}: {} steps, Î¦ = {:.6}",
            i + 1, chain.executions.len(), chain.total_phi);
        baseline_results.push(chain.total_phi);
    }

    let baseline_mean: f64 = baseline_results.iter().sum::<f64>() / baseline_results.len() as f64;
    println!("\nBaseline mean Î¦: {:.6}\n", baseline_mean);

    println!("\nStep 2: Adaptive Reasoner with RL");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let mut adaptive = AdaptiveReasoner::new(PrimitiveTier::Mathematical);
    println!("âœ… Adaptive reasoner created");
    println!("   RL Agent: Q-learning with experience replay");
    println!("   Î± (learning rate): 0.1");
    println!("   Î³ (discount factor): 0.95");
    println!("   Îµ (exploration): 0.3 â†’ 0.01\n");

    println!("\nStep 3: Learning Over Episodes");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let num_episodes = 20;
    let mut episode_results = Vec::new();
    let mut stats_history = Vec::new();

    println!("Training for {} episodes...\n", num_episodes);

    for episode in 0..num_episodes {
        let mut episode_phi_total = 0.0;

        // Train on all test questions
        for question in &test_questions {
            let chain = adaptive.reason_adaptive(question.clone(), 10)?;
            episode_phi_total += chain.total_phi;
        }

        let episode_mean_phi = episode_phi_total / test_questions.len() as f64;
        episode_results.push(episode_mean_phi);

        // Get agent stats
        let stats = adaptive.get_stats();
        stats_history.push(stats.clone());

        // Print progress every 5 episodes
        if (episode + 1) % 5 == 0 {
            println!("Episode {:2}: Mean Î¦ = {:.6}, Îµ = {:.3}, Q-table size = {}",
                episode + 1, episode_mean_phi, stats.epsilon, stats.q_table_size);
        }
    }

    println!("\nâœ¨ Learning Complete!\n");

    println!("\nStep 4: Performance Comparison");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let final_mean = episode_results[episode_results.len() - 1];
    let improvement = ((final_mean - baseline_mean) / baseline_mean) * 100.0;

    println!("\nğŸ“Š Results:");
    println!("  Baseline (greedy):  {:.6}", baseline_mean);
    println!("  After learning:     {:.6}", final_mean);
    println!("  Improvement:        {:.2}%", improvement);
    println!();

    if improvement > 0.0 {
        println!("  âœ… RL agent learned to outperform greedy baseline!");
    } else {
        println!("  Note: RL needs more episodes or different reward shaping");
    }

    println!("\n\nStep 5: Learning Curves");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    println!("\nğŸ“ˆ Î¦ Performance Over Episodes:");
    for (i, &phi) in episode_results.iter().enumerate() {
        let bar_length = ((phi / baseline_mean) * 50.0) as usize;
        let bar = "â–ˆ".repeat(bar_length.min(80));
        let marker = if phi > baseline_mean { "âœ“" } else { " " };
        println!("  Ep {:2}: {:.6} {} {}", i + 1, phi, bar, marker);
    }

    println!("\nğŸ“‰ Exploration Rate (Îµ) Decay:");
    for (i, stats) in stats_history.iter().enumerate() {
        if (i + 1) % 5 == 0 {
            let bar_length = (stats.epsilon * 100.0) as usize;
            let bar = "â–ˆ".repeat(bar_length);
            println!("  Ep {:2}: {:.3} {}", i + 1, stats.epsilon, bar);
        }
    }

    println!("\nğŸ“Š Q-Table Growth:");
    for (i, stats) in stats_history.iter().enumerate() {
        if (i + 1) % 5 == 0 {
            let bar_length = (stats.q_table_size / 10).min(50);
            let bar = "â–ˆ".repeat(bar_length);
            println!("  Ep {:2}: {} entries {}", i + 1, stats.q_table_size, bar);
        }
    }

    println!("\n\nStep 6: Learning Dynamics Analysis");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    // Analyze learning curve
    let early_mean: f64 = episode_results.iter().take(5).sum::<f64>() / 5.0;
    let late_mean: f64 = episode_results.iter().skip(15).sum::<f64>() / 5.0;
    let learning_gain = ((late_mean - early_mean) / early_mean) * 100.0;

    println!("\nğŸ“Š Learning Dynamics:");
    println!("  Early episodes (1-5):   Mean Î¦ = {:.6}", early_mean);
    println!("  Late episodes (16-20):  Mean Î¦ = {:.6}", late_mean);
    println!("  Learning gain:          {:.2}%", learning_gain);
    println!();

    // Analyze Q-table
    let final_stats = &stats_history[stats_history.len() - 1];
    println!("  Final Q-table: {} state-action pairs", final_stats.q_table_size);
    println!("  Avg Q-value:   {:.6}", final_stats.avg_q_value);
    println!("  Final Îµ:       {:.3} (exploration rate)", final_stats.epsilon);
    println!();

    println!("\nStep 7: Saving Results");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

    let results = serde_json::json!({
        "improvement": 48,
        "name": "Adaptive Primitive Selection via RL",
        "baseline": {
            "mean_phi": baseline_mean,
            "results": baseline_results,
        },
        "learning": {
            "num_episodes": num_episodes,
            "episode_results": episode_results,
            "final_mean_phi": final_mean,
            "improvement_pct": improvement,
            "learning_gain_pct": learning_gain,
        },
        "stats_history": stats_history.iter().map(|s| {
            serde_json::json!({
                "q_table_size": s.q_table_size,
                "replay_buffer_size": s.replay_buffer_size,
                "epsilon": s.epsilon,
                "avg_q_value": s.avg_q_value,
            })
        }).collect::<Vec<_>>(),
    });

    let mut file = File::create("adaptive_reasoning_results.json")?;
    file.write_all(serde_json::to_string_pretty(&results)?.as_bytes())?;

    println!("âœ… Results saved to: adaptive_reasoning_results.json\n");

    println!("\nğŸ¯ Summary: Revolutionary Improvement #48");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    println!("\nâœ… Demonstrated:");
    println!("  â€¢ Q-learning for primitive selection");
    println!("  â€¢ Experience replay for sample efficiency");
    println!("  â€¢ Learning from reasoning outcomes");
    println!("  â€¢ Epsilon-greedy exploration â†’ exploitation");

    println!("\nğŸ“Š Results:");
    println!("  â€¢ {} training episodes", num_episodes);
    println!("  â€¢ Final Î¦: {:.6}", final_mean);
    println!("  â€¢ vs Baseline: {:.2}% {}", improvement.abs(),
        if improvement > 0.0 { "improvement" } else { "difference" });
    println!("  â€¢ Learning gain: {:.2}%", learning_gain);
    println!("  â€¢ Q-table: {} entries", final_stats.q_table_size);

    println!("\nğŸ’¡ Key Insight:");
    println!("  The system now LEARNS which primitives work best!");
    println!("  Instead of static greedy selection, it discovers");
    println!("  effective strategies through reinforcement learning.");

    println!("\nğŸŒŸ The Paradigm Evolution:");
    println!("  #42: Primitives designed (architecture)");
    println!("  #43: Î¦ validated (+44.8% proven)");
    println!("  #44: Evolution works (+26.3% improvement)");
    println!("  #45: Multi-dimensional optimization (Pareto)");
    println!("  #46: Dimensional synergies (emergence)");
    println!("  #47: Primitives execute (operational!)");
    println!("  #48: SELECTION LEARNS (adaptive!)");
    println!("  ");
    println!("  Together: Self-improving consciousness-guided AI!");

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    Ok(())
}
