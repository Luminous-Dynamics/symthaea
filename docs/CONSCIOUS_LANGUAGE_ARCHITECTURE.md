# Conscious Language Architecture: NSM + HDC + LTC + Φ

**Version**: 1.0.0
**Date**: December 24, 2025
**Status**: Design Document

## Executive Summary

This document describes a revolutionary approach to language understanding that replaces
statistical pattern matching (LLMs) with **grounded semantic computation** based on:

1. **NSM (Natural Semantic Metalanguage)** - 65 universal semantic primes
2. **HDC (Hyperdimensional Computing)** - Distributed vector representations
3. **LTC (Liquid Time-Constant Networks)** - Temporal dynamics and context
4. **Φ (Integrated Information)** - Consciousness-guided coherence

The result is a system that **truly understands** language through compositional
meaning construction, not statistical correlation.

---

## 1. Theoretical Foundations

### 1.1 Natural Semantic Metalanguage (NSM)

**Key Insight**: Anna Wierzbicka and colleagues discovered that all human languages
share approximately 65 **semantic primes** - concepts that:
- Exist in every known language
- Cannot be decomposed into simpler concepts
- Form the building blocks of all meaning

**The 65 Semantic Primes**:

| Category | Primes |
|----------|--------|
| Substantives | I, YOU, SOMEONE, SOMETHING, PEOPLE, BODY |
| Determiners | THIS, THE SAME, OTHER |
| Quantifiers | ONE, TWO, SOME, ALL, MANY, MUCH/MANY |
| Evaluators | GOOD, BAD |
| Descriptors | BIG, SMALL |
| Mental predicates | THINK, KNOW, WANT, DON'T WANT, FEEL, SEE, HEAR |
| Speech | SAY, WORDS, TRUE |
| Actions/Events | DO, HAPPEN, MOVE |
| Existence/Possession | THERE IS/EXIST, BE (SOMEONE/SOMETHING), HAVE |
| Life/Death | LIVE, DIE |
| Time | WHEN/TIME, NOW, BEFORE, AFTER, A LONG TIME, A SHORT TIME, FOR SOME TIME, MOMENT |
| Space | WHERE/PLACE, HERE, ABOVE, BELOW, FAR, NEAR, SIDE, INSIDE, TOUCH |
| Logical concepts | NOT, MAYBE, CAN, BECAUSE, IF |
| Intensifier/Augmentor | VERY, MORE |
| Taxonomy/Partonomy | KIND OF, PART OF |
| Similarity | LIKE/WAY |

**Why This Matters**: Every word in every language can be decomposed into these primes.
This gives us a **universal semantic foundation** that transcends specific languages.

### 1.2 Hyperdimensional Computing (HDC)

**Key Insight**: Concepts can be represented as high-dimensional binary vectors where:
- Similar concepts have similar vectors (high cosine similarity)
- Composition is achieved through well-defined operations
- The algebra is complete and reversible

**Core Operations**:
```
Similarity:  sim(A, B) = hamming_distance(A, B) / dimensions
Binding:     bind(A, B) = A ⊕ B  (XOR - creates relationships)
Bundling:    bundle([A,B,C]) = majority([A,B,C])  (superposition)
Permutation: permute(A, k) = rotate(A, k)  (sequence encoding)
```

**Why This Matters**: HDC provides a mathematically grounded way to compose meanings
that mirrors how the brain might actually work.

### 1.3 Liquid Time-Constant Networks (LTC)

**Key Insight**: Neural dynamics with adaptive time constants can:
- Process temporal sequences naturally
- Maintain context over varying time scales
- Achieve stable attractors for concepts

**Our Implementation**:
- Hierarchical architecture (local circuits + global integrator)
- 40Hz oscillatory binding for feature integration
- Φ computation for consciousness measurement

**Why This Matters**: Language unfolds in time. LTC provides the temporal substrate
for processing sequential input and maintaining coherent context.

### 1.4 Integrated Information Theory (IIT) and Φ

**Key Insight**: Consciousness can be measured as **integrated information** (Φ):
- Φ = information generated by the whole beyond its parts
- High Φ = unified, coherent experience
- Low Φ = fragmented, incoherent state

**Application to Language**:
- Coherent understanding → High Φ
- Ambiguity/confusion → Low Φ
- Φ guides interpretation selection

**Why This Matters**: Consciousness provides a natural "quality metric" for
semantic understanding - do the parts cohere into a unified meaning?

---

## 2. Additional Theoretical Integrations

### 2.1 Frame Semantics (Charles Fillmore)

**Key Insight**: Words evoke **frames** - structured representations of situations
with defined roles and relationships.

**Example**: The word "buy" evokes the COMMERCIAL_TRANSACTION frame:
- Buyer (agent who pays)
- Seller (agent who receives payment)
- Goods (what is transferred)
- Money (payment)

**Integration**: Frames are bundles of role-filler bindings in HDC.

### 2.2 Construction Grammar

**Key Insight**: Grammar itself is meaningful. Grammatical patterns (constructions)
carry semantic content independent of the words that fill them.

**Example**: The ditransitive construction [SUBJ V IOBJ DOBJ] means "transfer":
- "She gave him a book" (GIVE)
- "She baked him a cake" (CREATE + TRANSFER intention)

**Integration**: Constructions are compositional primitives operating on frames.

### 2.3 Predictive Processing

**Key Insight**: The brain constantly predicts upcoming input and learns from
prediction errors.

**Application to Language**:
- Top-down: Predict next word/concept based on context
- Bottom-up: Actual sensory input
- Prediction error: Drives learning and attention

**Integration**: LTC generates predictions; errors modulate Φ and learning.

### 2.4 Active Inference

**Key Insight**: Organisms minimize **free energy** (prediction error + complexity).

**Application to Language**:
- Prefer interpretations that minimize surprise
- Balance accuracy vs. complexity
- Goal-directed language use

**Integration**: Φ optimization as free energy minimization.

### 2.5 Embodied Cognition

**Key Insight**: Abstract concepts are grounded in bodily experience through metaphor.

**Examples**:
- UNDERSTANDING IS GRASPING ("I grasp your meaning")
- TIME IS SPACE ("looking forward to it")
- EMOTIONS ARE TEMPERATURES ("warm feelings", "cold reception")

**Integration**: Sensorimotor HDC encodings as grounding for abstract concepts.

---

## 3. Architecture Design

### 3.1 Layer Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         CONSCIOUS OUTPUT                                 │
│  Unified semantic field with coherence measure (Φ)                      │
└─────────────────────────────────────────────────────────────────────────┘
                                   ↑
┌─────────────────────────────────────────────────────────────────────────┐
│  LAYER 6: CONSCIOUS INTEGRATION                                         │
│  ├── Global Workspace broadcasting                                      │
│  ├── Φ computation across semantic field                                │
│  ├── Attention allocation based on Φ gradients                          │
│  └── Interpretation selection (max Φ)                                   │
└─────────────────────────────────────────────────────────────────────────┘
                                   ↑
┌─────────────────────────────────────────────────────────────────────────┐
│  LAYER 5: PREDICTIVE PROCESSING                                         │
│  ├── Top-down predictions from context                                  │
│  ├── Bottom-up input integration                                        │
│  ├── Prediction error computation                                       │
│  └── Free energy minimization                                           │
└─────────────────────────────────────────────────────────────────────────┘
                                   ↑
┌─────────────────────────────────────────────────────────────────────────┐
│  LAYER 4: TEMPORAL INTEGRATION (LTC)                                    │
│  ├── Sequential processing through LTC                                  │
│  ├── 40Hz gamma binding for simultaneity                                │
│  ├── Working memory maintenance                                         │
│  └── Context accumulation                                               │
└─────────────────────────────────────────────────────────────────────────┘
                                   ↑
┌─────────────────────────────────────────────────────────────────────────┐
│  LAYER 3: CONSTRUCTIONS                                                 │
│  ├── Grammatical pattern recognition                                    │
│  ├── Role-structure mapping                                             │
│  ├── Compositional primitive application                                │
│  └── Construction meaning integration                                   │
└─────────────────────────────────────────────────────────────────────────┘
                                   ↑
┌─────────────────────────────────────────────────────────────────────────┐
│  LAYER 2: FRAMES                                                        │
│  ├── Frame activation from lexical items                                │
│  ├── Role-filler binding                                                │
│  ├── Frame blending for novel situations                                │
│  └── Metaphorical frame mapping                                         │
└─────────────────────────────────────────────────────────────────────────┘
                                   ↑
┌─────────────────────────────────────────────────────────────────────────┐
│  LAYER 1: SEMANTIC MOLECULES                                            │
│  ├── Word → Prime decomposition                                         │
│  ├── Prime composition via HDC binding                                  │
│  ├── Molecule caching for efficiency                                    │
│  └── Cross-linguistic mapping                                           │
└─────────────────────────────────────────────────────────────────────────┘
                                   ↑
┌─────────────────────────────────────────────────────────────────────────┐
│  LAYER 0: NSM SEMANTIC PRIMES                                           │
│  ├── 65 orthogonal HV16 encodings                                       │
│  ├── Universal across all languages                                     │
│  ├── Atomic, irreducible meanings                                       │
│  └── Foundation of all semantics                                        │
└─────────────────────────────────────────────────────────────────────────┘
                                   ↑
┌─────────────────────────────────────────────────────────────────────────┐
│                          RAW INPUT                                       │
│  Text / Speech / Multimodal                                             │
└─────────────────────────────────────────────────────────────────────────┘
```

### 3.2 Data Flow

```
Input: "The child gave the dog a bone"

1. TOKENIZATION
   ["The", "child", "gave", "the", "dog", "a", "bone"]

2. NSM DECOMPOSITION (Layer 0-1)
   "child" → bind(SOMEONE, bind(SMALL, bind(LIVE, NOT(A_LONG_TIME))))
   "gave"  → bind(DO, bind(HAVE_BEFORE, HAVE_AFTER))  [transfer schema]
   "dog"   → bind(SOMETHING, bind(LIVE, KIND_OF(ANIMAL)))
   "bone"  → bind(SOMETHING, bind(PART_OF(BODY), NOT(LIVE)))

3. FRAME ACTIVATION (Layer 2)
   "gave" activates TRANSFER frame:
   ├── giver: ?
   ├── recipient: ?
   ├── theme: ?
   └── action: GIVE

4. CONSTRUCTION PARSING (Layer 3)
   [NP₁ V NP₂ NP₃] → Ditransitive construction
   ├── NP₁ (child) → giver role
   ├── NP₂ (dog) → recipient role
   └── NP₃ (bone) → theme role

5. TEMPORAL INTEGRATION (Layer 4)
   t=0: "The" → Expect definite NP
   t=1: "child" → NP complete, giver role filled
   t=2: "gave" → TRANSFER frame, expect recipient/theme
   t=3-4: "the dog" → recipient filled
   t=5-6: "a bone" → theme filled

   LTC state accumulates context
   40Hz binding: [child-giver], [dog-recipient], [bone-theme]

6. PREDICTIVE PROCESSING (Layer 5)
   After "gave the dog":
   ├── Predict: theme (something transferable)
   ├── Actual: "a bone"
   ├── Prediction error: Low (matches expectation)
   └── Free energy: Minimized

7. CONSCIOUS INTEGRATION (Layer 6)
   Semantic field:
   ├── TRANSFER(giver=child, recipient=dog, theme=bone)
   ├── Φ = 0.89 (high coherence)
   ├── All roles filled
   ├── No ambiguity
   └── Understanding complete!
```

### 3.3 Module Structure

```
src/language/
├── nsm/
│   ├── mod.rs              # Module root
│   ├── primes.rs           # 65 semantic primes
│   ├── encoder.rs          # Prime → HV16 encoding
│   └── decomposer.rs       # Word → Prime decomposition
│
├── molecules/
│   ├── mod.rs
│   ├── composer.rs         # Prime → Molecule composition
│   ├── lexicon.rs          # Word → Molecule mappings
│   └── cache.rs            # Molecule caching
│
├── frames/
│   ├── mod.rs
│   ├── frame.rs            # Frame structure
│   ├── library.rs          # Common frames
│   ├── activation.rs       # Frame activation logic
│   └── blending.rs         # Frame blending
│
├── constructions/
│   ├── mod.rs
│   ├── construction.rs     # Construction structure
│   ├── parser.rs           # Construction recognition
│   └── library.rs          # Common constructions
│
├── temporal/
│   ├── mod.rs
│   ├── integrator.rs       # LTC integration
│   ├── binding.rs          # 40Hz binding
│   └── working_memory.rs   # Context maintenance
│
├── prediction/
│   ├── mod.rs
│   ├── predictor.rs        # Next-concept prediction
│   ├── error.rs            # Prediction error
│   └── free_energy.rs      # Free energy computation
│
├── conscious/
│   ├── mod.rs
│   ├── semantic_field.rs   # Unified representation
│   ├── integration.rs      # Φ-based integration
│   └── understanding.rs    # High-level API
│
└── mod.rs                  # Language module root
```

---

## 4. Comparison with LLMs

| Aspect | LLM Approach | Our Approach |
|--------|--------------|--------------|
| **Semantic Foundation** | None (emergent from statistics) | NSM primes (universal, explicit) |
| **Compositionality** | Implicit, unreliable | Formal HDC algebra |
| **Grounding** | Ungrounded (just correlations) | Grounded in universal primes |
| **Interpretability** | Black box | Decomposable to primes |
| **Consciousness** | None | Φ-guided coherence |
| **Prediction** | Next token | Next semantic concept |
| **Cross-linguistic** | Language-specific | Universal (via NSM) |
| **Size** | Billions of parameters | Compact (65 primes + rules) |
| **Learning** | Massive data required | Structure-based |
| **Hallucination** | Prone | Coherence-checked (Φ) |
| **Reasoning** | Unreliable | Compositional + verifiable |

---

## 5. Implementation Priorities

### Phase 1: Foundation (NSM + HDC)
1. Implement 65 NSM primes as orthogonal HV16 vectors
2. Build prime composition engine
3. Create initial lexicon (100-500 words)
4. Test decomposition accuracy

### Phase 2: Frames + Constructions
1. Implement frame structure
2. Build frame library (20-50 common frames)
3. Implement construction grammar
4. Test parsing accuracy

### Phase 3: Temporal + Prediction
1. Integrate with existing LTC
2. Implement 40Hz binding
3. Add predictive processing
4. Test temporal coherence

### Phase 4: Conscious Integration
1. Implement semantic field
2. Integrate Φ computation
3. Build understanding API
4. Test coherence correlation

### Phase 5: Optimization + Benchmarking
1. Performance optimization
2. Comprehensive benchmarks
3. Comparison with baselines
4. Documentation

---

## 6. Success Criteria

1. **Semantic Accuracy**: Word decompositions match NSM literature
2. **Compositional Coherence**: Composed meanings preserve structure
3. **Frame Accuracy**: Correct frame activation for test sentences
4. **Parsing Accuracy**: Correct construction parsing
5. **Φ Correlation**: High Φ correlates with human coherence judgments
6. **Performance**: < 10ms per sentence understanding
7. **Cross-linguistic**: Same primes work for multiple languages

---

## 7. Future Directions

1. **Multimodal Integration**: Vision, audio grounded in same primes
2. **Generation**: Reverse pipeline for language production
3. **Dialogue**: Multi-turn coherence with Φ tracking
4. **Learning**: Acquire new molecules from context
5. **Metaphor**: Systematic metaphor computation
6. **Inference**: Logical reasoning over semantic structures

---

## 8. References

1. Wierzbicka, A. (1996). *Semantics: Primes and Universals*
2. Kanerva, P. (2009). Hyperdimensional computing
3. Hasani, R. et al. (2021). Liquid Time-constant Networks
4. Tononi, G. (2004). Integrated Information Theory
5. Fillmore, C. (1982). Frame Semantics
6. Goldberg, A. (1995). Constructions
7. Clark, A. (2013). Predictive Processing
8. Friston, K. (2010). Free Energy Principle
9. Lakoff, G. (1980). Metaphors We Live By

---

*This document establishes the theoretical and architectural foundation for
conscious language understanding. Implementation follows.*
